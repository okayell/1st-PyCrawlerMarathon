{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "headers = {'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36 Edg/80.0.361.48'}\n",
    "cookies = {'over18': '1'}\n",
    "url = 'https://www.ptt.cc/bbs/Gossiping/index.html'\n",
    "\n",
    "res = requests.get(url, headers=headers, cookies=cookies)\n",
    "res.encoding = 'utf-8'\n",
    "soup = BeautifulSoup(res.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = []\n",
    "for article in soup.find_all('div', class_='r-ent'):\n",
    "    info = {}\n",
    "    info['title'] = article.find('div', class_='title').text\n",
    "    info['author'] = article.find('div', class_='author').text\n",
    "    url = 'https://www.ptt.cc/' + article.find('a')['href']\n",
    "    info['url'] = url\n",
    "    soup2 = BeautifulSoup(requests.get(url, headers=headers, cookies=cookies).text, 'html.parser')\n",
    "    main_content = soup2.find('div', id='main-content')\n",
    "    info['pub_time'] = main_content.find_all('span', class_='article-meta-value')[3].text\n",
    "    for i in main_content.find_all('span', class_='f2'):\n",
    "        if '來自' in str(i):\n",
    "            info['author_ip'] = str(i).split('來自: ')[1].split(' (')[0]\n",
    "    info['content'] = main_content.text.split('2020')[1].split('發信站')[0].strip()\n",
    "    datas.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>url</th>\n",
       "      <th>pub_time</th>\n",
       "      <th>author_ip</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nRe: [爆卦] 轉彎啦！對岸親人無照顧能力、未成年才可回來\\n</td>\n",
       "      <td>Morrislakbay</td>\n",
       "      <td>https://www.ptt.cc//bbs/Gossiping/M.1581422545...</td>\n",
       "      <td>Tue Feb 11 20:02:22 2020</td>\n",
       "      <td>223.140.224.192</td>\n",
       "      <td>以我對民進黨的瞭解\\n\\n這一定是故意的\\n\\n\\n前面先放風聲 激起民怨\\n\\n後面在縮小...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n[問卦] 陸配來台又不能公布住所是要怎麼防疫\\n</td>\n",
       "      <td>ElectronPair</td>\n",
       "      <td>https://www.ptt.cc//bbs/Gossiping/M.1581422566...</td>\n",
       "      <td>Tue Feb 11 20:02:44 2020</td>\n",
       "      <td>42.75.251.251</td>\n",
       "      <td>我說全臺灣大部分人都不支持你開放那麼多人一次來台\\n居家隔離14天，真的有人會乖乖隔離嗎？\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nRe: [新聞] 陸委會宣布「開放陸配來台探親」！　入\\n</td>\n",
       "      <td>lastpost</td>\n",
       "      <td>https://www.ptt.cc//bbs/Gossiping/M.1581422603...</td>\n",
       "      <td>Tue Feb 11 20:03:21 2020</td>\n",
       "      <td>114.136.31.108</td>\n",
       "      <td>這就是為那些可以上達天聽的人士開的後門啦\\n\\n畢竟情人節要到了\\n\\n其實這議題的民意很清...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nRe: [爆卦] 廣東省實行無產階級革命了\\n</td>\n",
       "      <td>Lilim</td>\n",
       "      <td>https://www.ptt.cc//bbs/Gossiping/M.1581422620...</td>\n",
       "      <td>Tue Feb 11 20:03:38 2020</td>\n",
       "      <td>223.137.207.57</td>\n",
       "      <td>※ 引述《terrymoon (說好的幸福呢)》之銘言：\\n: 廣東省緊急立法授權政府可徵用...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nRe: [爆卦] 日本武漢肺炎將可住進一般病房\\n</td>\n",
       "      <td>Wardyal</td>\n",
       "      <td>https://www.ptt.cc//bbs/Gossiping/M.1581422625...</td>\n",
       "      <td>Tue Feb 11 20:03:43 2020</td>\n",
       "      <td>114.198.160.22</td>\n",
       "      <td>啊\\n\\n所以日本台灣到底有沒有死人\\n\\n因為目前看中國那邊好像死很多人\\n\\n不清楚原因...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                title        author  \\\n",
       "0  \\nRe: [爆卦] 轉彎啦！對岸親人無照顧能力、未成年才可回來\\n  Morrislakbay   \n",
       "1          \\n[問卦] 陸配來台又不能公布住所是要怎麼防疫\\n  ElectronPair   \n",
       "2     \\nRe: [新聞] 陸委會宣布「開放陸配來台探親」！　入\\n      lastpost   \n",
       "3           \\nRe: [爆卦] 廣東省實行無產階級革命了\\n         Lilim   \n",
       "4         \\nRe: [爆卦] 日本武漢肺炎將可住進一般病房\\n       Wardyal   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.ptt.cc//bbs/Gossiping/M.1581422545...   \n",
       "1  https://www.ptt.cc//bbs/Gossiping/M.1581422566...   \n",
       "2  https://www.ptt.cc//bbs/Gossiping/M.1581422603...   \n",
       "3  https://www.ptt.cc//bbs/Gossiping/M.1581422620...   \n",
       "4  https://www.ptt.cc//bbs/Gossiping/M.1581422625...   \n",
       "\n",
       "                   pub_time        author_ip  \\\n",
       "0  Tue Feb 11 20:02:22 2020  223.140.224.192   \n",
       "1  Tue Feb 11 20:02:44 2020    42.75.251.251   \n",
       "2  Tue Feb 11 20:03:21 2020   114.136.31.108   \n",
       "3  Tue Feb 11 20:03:38 2020   223.137.207.57   \n",
       "4  Tue Feb 11 20:03:43 2020   114.198.160.22   \n",
       "\n",
       "                                             content  \n",
       "0  以我對民進黨的瞭解\\n\\n這一定是故意的\\n\\n\\n前面先放風聲 激起民怨\\n\\n後面在縮小...  \n",
       "1  我說全臺灣大部分人都不支持你開放那麼多人一次來台\\n居家隔離14天，真的有人會乖乖隔離嗎？\\...  \n",
       "2  這就是為那些可以上達天聽的人士開的後門啦\\n\\n畢竟情人節要到了\\n\\n其實這議題的民意很清...  \n",
       "3  ※ 引述《terrymoon (說好的幸福呢)》之銘言：\\n: 廣東省緊急立法授權政府可徵用...  \n",
       "4  啊\\n\\n所以日本台灣到底有沒有死人\\n\\n因為目前看中國那邊好像死很多人\\n\\n不清楚原因...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(datas)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'以我對民進黨的瞭解\\n\\n這一定是故意的\\n\\n\\n前面先放風聲 激起民怨\\n\\n後面在縮小開發範圍\\n\\n\\n這麼熟煉  一定是故意的\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n嘻嘻\\n\\n--\\n※'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我/ 對民進/ 黨/ 的/ 瞭解/ \n",
      "/ \n",
      "/ 這/ 一定/ 是/ 故意/ 的/ \n",
      "/ \n",
      "/ \n",
      "/ 前面/ 先放風聲/  / 激起/ 民怨/ \n",
      "/ \n",
      "/ 後/ 面/ 在/ 縮小開/ 發範圍/ \n",
      "/ \n",
      "/ \n",
      "/ 這麼/ 熟煉/  /  / 一定/ 是/ 故意/ 的/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ \n",
      "/ 嘻嘻/ \n",
      "/ \n",
      "/ --/ \n",
      "/ ※\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "import jieba.analyse\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "text = df['content'][0]\n",
    "seg_list = jieba.cut(text, cut_all=False)\n",
    "for seg in seg_list:\n",
    "    print(\"/ \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 移除停留詞\n",
    "def remove_stop_words(file_name, seg_list):\n",
    "    new_list = []\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        stop_words = f.readlines()\n",
    "    stop_words = [stop_word.rstrip() for stop_word in stop_words]\n",
    "\n",
    "    for seg in seg_list:\n",
    "        if seg not in stop_words:\n",
    "            new_list.append(seg) \n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'stop_word.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-390a7cca296e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'stop_word.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mseg_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_stop_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseg_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseg_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'remove_stop_words: '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseg_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-c4f1dec1216b>\u001b[0m in \u001b[0;36mremove_stop_words\u001b[1;34m(file_name, seg_list)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremove_stop_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseg_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mnew_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstop_word\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mstop_word\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'stop_word.txt'"
     ]
    }
   ],
   "source": [
    "file_name = 'stop_word.txt'\n",
    "seg_list = remove_stop_words(file_name, seg_list)\n",
    "for i in seg_list:\n",
    "    print(i, end='')\n",
    "print('remove_stop_words: '.join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
